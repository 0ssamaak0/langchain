{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama.cpp\n",
    "\n",
    "[llama-cpp-python](https://github.com/abetlen/llama-cpp-python) is a Python binding for [llama.cpp](https://github.com/ggerganov/llama.cpp). \n",
    "It supports [several LLMs](https://github.com/ggerganov/llama.cpp).\n",
    "\n",
    "This notebook goes over how to run `llama-cpp-python` within LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "There are different options on how to install the llama-cpp package: \n",
    "- only CPU usage\n",
    "- CPU + GPU (using one of many BLAS backends)\n",
    "- Metal GPU (MacOS with Apple Silicon Chip) \n",
    "\n",
    "### CPU only installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation with OpenBLAS / cuBLAS / CLBlast\n",
    "\n",
    "`lama.cpp` supports multiple BLAS backends for faster processing. Use the `FORCE_CMAKE=1` environment variable to force the use of cmake and install the pip package for the desired BLAS backend ([source](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast)).\n",
    "\n",
    "Example installation with cuBLAS backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: If you have already installed the CPU only version of the package, you need to reinstall it from scratch. Consider the following command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation with Metal\n",
    "\n",
    "`llama.cpp` supports Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks. Use the `FORCE_CMAKE=1` environment variable to force the use of cmake and install the pip package for the Metal support ([source](https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md)).\n",
    "\n",
    "Example installation with Metal Support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: If you have already installed a cpu only version of the package, you need to reinstall it from scratch: consider the following command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation with Windows\n",
    "\n",
    "It is stable to install the `llama-cpp-python` library by compiling from the source. You can follow most of the instructions in the repository itself but there are some windows specific instructions which might be useful.\n",
    "\n",
    "Requirements to install the `llama-cpp-python`,\n",
    "\n",
    "- git\n",
    "- python\n",
    "- cmake\n",
    "- Visual Studio Community (make sure you install this with the following settings)\n",
    "    - Desktop development with C++\n",
    "    - Python development\n",
    "    - Linux embedded development with C++\n",
    "\n",
    "1. Clone git repository recursively to get `llama.cpp` submodule as well \n",
    "\n",
    "```\n",
    "git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.git\n",
    "```\n",
    "\n",
    "2. Open up command Prompt (or anaconda prompt if you have it installed), set up environment variables to install. Follow this if you do not have a GPU, you must set both of the following variables.\n",
    "\n",
    "```\n",
    "set FORCE_CMAKE=1\n",
    "set CMAKE_ARGS=-DLLAMA_CUBLAS=OFF\n",
    "```\n",
    "You can ignore the second environment variable if you have an NVIDIA GPU.\n",
    "\n",
    "#### Compiling and installing\n",
    "\n",
    "In the same command prompt (anaconda prompt) you set the variables, you can `cd` into `llama-cpp-python` directory and run the following commands.\n",
    "\n",
    "```\n",
    "python setup.py clean\n",
    "python setup.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are following all instructions to [install all necessary model files](https://github.com/ggerganov/llama.cpp).\n",
    "\n",
    "You don't need an `API_TOKEN` as you will run the LLM locally.\n",
    "\n",
    "It is worth understanding which models are suitable to be used on the desired machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider using a template that suits your model! Check the models page on HuggingFace etc. to get a correct prompting template.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using a LLaMA 2 7B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama/llama-2-7b-ggml/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stephen Colbert:\n",
      "Yo, John, I heard you've been talkin' smack about me on your show.\n",
      "Let me tell you somethin', pal, I'm the king of late-night TV\n",
      "My satire is sharp as a razor, it cuts deeper than a knife\n",
      "While you're just a british bloke tryin' to be funny with your accent and your wit.\n",
      "John Oliver:\n",
      "Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\n",
      "My show is the one that people actually watch and listen to, not just for the laughs but for the facts.\n",
      "While you're busy talkin' trash, I'm out here bringing the truth to light.\n",
      "Stephen Colbert:\n",
      "Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.\n",
      "You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\n",
      "While I'm the one who's really makin' a difference, with my sat"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   358.60 ms\n",
      "llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)\n",
      "llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)\n",
      "llama_print_timings:       total time = 11332.41 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nStephen Colbert:\\nYo, John, I heard you've been talkin' smack about me on your show.\\nLet me tell you somethin', pal, I'm the king of late-night TV\\nMy satire is sharp as a razor, it cuts deeper than a knife\\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\\nJohn Oliver:\\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\\nStephen Colbert:\\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\\nWhile I'm the one who's really makin' a difference, with my sat\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: A rap battle between Stephen Colbert and John Oliver\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using a LLaMA v1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. First, find out when Justin Bieber was born.\n",
      "2. We know that Justin Bieber was born on March 1, 1994.\n",
      "3. Next, we need to look up when the Super Bowl was played in that year.\n",
      "4. The Super Bowl was played on January 28, 1995.\n",
      "5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   434.15 ms\n",
      "llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)\n",
      "llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)\n",
      "llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)\n",
      "llama_print_timings:       total time = 28945.95 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. First, find out when Justin Bieber was born.\\n2. We know that Justin Bieber was born on March 1, 1994.\\n3. Next, we need to look up when the Super Bowl was played in that year.\\n4. The Super Bowl was played on January 28, 1995.\\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "If the installation with BLAS backend was correct, you will see a `BLAS = 1` indicator in model properties.\n",
    "\n",
    "Two of the most important parameters for use with GPU are:\n",
    "\n",
    "- `n_gpu_layers` - determines how many layers of the model are offloaded to your GPU.\n",
    "- `n_batch` - how many tokens are processed in parallel. \n",
    "\n",
    "Setting these parameters correctly will dramatically improve the evaluation speed (see [wrapper code](https://github.com/mmagnesium/langchain/blob/master/langchain/llms/llamacpp.py) for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 6983.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x138a4d4e0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x13930d740\n",
      "ggml_metal_init: loaded kernel_mul                            0x139005650\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x138a43dc0\n",
      "ggml_metal_init: loaded kernel_scale                          0x138a26ed0\n",
      "ggml_metal_init: loaded kernel_silu                           0x138a557f0\n",
      "ggml_metal_init: loaded kernel_relu                           0x13930da00\n",
      "ggml_metal_init: loaded kernel_gelu                           0x1380fd640\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x138a32f00\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x138a5d130\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x138a5d390\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13930dc60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x139006280\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x138a5d5f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13930e590\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13930ed00\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13930fcc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x139310450\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x138a5d850\n",
      "ggml_metal_init: loaded kernel_norm                           0x138a5dab0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x139005fe0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x138a5df70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13930f630\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x138a5e430\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x149607280\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x138a5e690\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x139311650\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13962c610\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x159606520\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x138a5ee10\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x1596b10d0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x1596b1890\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x1396bd930\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x1596b1d90\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x1596b1ff0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x1396df070\n",
      "ggml_metal_init: loaded kernel_rope                           0x1596b2290\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1596b2890\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1596b2e60\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x1396df6a0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x1396dfc70\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   91.35 MB\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.50 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.36 MB, ( 6985.86 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7387.86 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    90.02 MB, ( 7477.88 / 21845.34)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Justin Bieber was born on March 1, 1994. The Super Bowl is played at the end of the NFL season, usually in early February. So let's assume that Justin Bieber was born in 1994 and the Super Bowl was played in early February of that year.\n",
      "\n",
      "Here are the NFL teams that won the Super Bowl in the years preceding Justin Bieber's birth:\n",
      "\n",
      "* 1993: The Dallas Cowboys\n",
      "* 1992: The Buffalo Bills\n",
      "* 1991: The Washington Redskins\n",
      "\n",
      "Since Justin Bieber was born in 1994, the NFL team that won the Super Bowl in the year he was born is...\n",
      "\n",
      "The Dallas Cowboys!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   570.37 ms\n",
      "llama_print_timings:      sample time =   121.58 ms /   174 runs   (    0.70 ms per token,  1431.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   570.24 ms /    45 tokens (   12.67 ms per token,    78.91 tokens per second)\n",
      "llama_print_timings:        eval time =  4820.00 ms /   173 runs   (   27.86 ms per token,    35.89 tokens per second)\n",
      "llama_print_timings:       total time =  5746.35 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nJustin Bieber was born on March 1, 1994. The Super Bowl is played at the end of the NFL season, usually in early February. So let's assume that Justin Bieber was born in 1994 and the Super Bowl was played in early February of that year.\\n\\nHere are the NFL teams that won the Super Bowl in the years preceding Justin Bieber's birth:\\n\\n* 1993: The Dallas Cowboys\\n* 1992: The Buffalo Bills\\n* 1991: The Washington Redskins\\n\\nSince Justin Bieber was born in 1994, the NFL team that won the Super Bowl in the year he was born is...\\n\\nThe Dallas Cowboys!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metal\n",
    "\n",
    "If the installation with Metal was correct, you will see a `NEON = 1` indicator in model properties.\n",
    "\n",
    "Two of the most important GPU parameters are:\n",
    "\n",
    "- `n_gpu_layers` - determines how many layers of the model are offloaded to your Metal GPU, in the most case, set it to `1` is enough for Metal\n",
    "- `n_batch` - how many tokens are processed in parallel, default is 8, set to bigger number.\n",
    "- `f16_kv` - for some reason, Metal only support `True`, otherwise you will get error such as `Asserting on type 0\n",
    "GGML_ASSERT: .../ggml-metal.m:706: false && \"not implemented\"`\n",
    "\n",
    "Setting these parameters correctly will dramatically improve the evaluation speed (see [wrapper code](https://github.com/mmagnesium/langchain/blob/master/langchain/llms/llamacpp.py) for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 6983.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x113b42480\n",
      "ggml_metal_init: loaded kernel_add_row                        0x113b44210\n",
      "ggml_metal_init: loaded kernel_mul                            0x113b43a80\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x113b44880\n",
      "ggml_metal_init: loaded kernel_scale                          0x113b45010\n",
      "ggml_metal_init: loaded kernel_silu                           0x113b45650\n",
      "ggml_metal_init: loaded kernel_relu                           0x113b427f0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x113b46300\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x113b46980\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x113b46e20\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x113b47860\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x113b48010\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x113b48880\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x113b48f70\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x113b49e00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x113b4a530\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x113b4ac70\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x113b4b3b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x113b4bb00\n",
      "ggml_metal_init: loaded kernel_norm                           0x113b4c1a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x113b4cba0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x113b4d360\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x113b4dba0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x113b4e560\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x113b4ed10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x113b4f580\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x113b4fdc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x113b50740\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x113b51250\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x113b51a80\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x113b522b0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x113b52ae0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x113b53310\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x113b53b40\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x113b54370\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x113b54ba0\n",
      "ggml_metal_init: loaded kernel_rope                           0x113b551a0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x113b55b10\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x113b56450\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x113b56dc0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x113b576b0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   91.35 MB\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.50 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.36 MB, ( 6985.86 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7387.86 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    90.02 MB, ( 7477.88 / 21845.34)AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The console log will show the following log to indicate Metal was enable properly.\n",
    "\n",
    "```\n",
    "ggml_metal_init: allocating\n",
    "ggml_metal_init: using MPS\n",
    "...\n",
    "```\n",
    "\n",
    "You also could check `Activity Monitor` by watching the GPU usage of the process, the CPU usage will drop dramatically after turn on `n_gpu_layers=1`. \n",
    "\n",
    "For the first call to the LLM, the performance may be slow due to the model compilation in Metal GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammers\n",
    "\n",
    "\n",
    "We can specify [grammers](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md) to constrain model outputs.\n",
    "\n",
    "Supply the path to the specifed `json.gbnf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 6983.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1100ce590\n",
      "ggml_metal_init: loaded kernel_add_row                        0x1100ce7f0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1100cea50\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x116dd92b0\n",
      "ggml_metal_init: loaded kernel_scale                          0x116dd9cb0\n",
      "ggml_metal_init: loaded kernel_silu                           0x1146bd720\n",
      "ggml_metal_init: loaded kernel_relu                           0x116d99ff0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x116dde300\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x116dde560\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x116dde7c0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x116ddec50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x116ddeeb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x116ddf110\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x116ddf370\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x116ddf5d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x116ddf830\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x116ddfa90\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x116ddfcf0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x116ddff50\n",
      "ggml_metal_init: loaded kernel_norm                           0x116de01b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x116de0410\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x116de0670\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x116de08d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x116de0b30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x116de0d90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x116de0ff0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x116de1250\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x116de14b0\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x116de1710\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x116de1970\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x116de1bd0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x116de1e30\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x116de2090\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x116de22f0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x116de2550\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x116de27b0\n",
      "ggml_metal_init: loaded kernel_rope                           0x116de2a10\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x116de2c70\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x116de2ed0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x116de3130\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x116de3390\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   91.35 MB\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, (21946.16 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.36 MB, (21947.52 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (22349.52 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    90.02 MB, (22439.53 / 21845.34), warning: current allocated size is greater than the recommended max working set size\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1 \n",
    "n_batch = 512 \n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    "    grammar_path=\"/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root ::= object EOF \n",
      "object ::= [{] hws object_12 [}] hws \n",
      "EOF ::= [<U+000A>] \n",
      "value ::= object | array | string | number | value_7 hws \n",
      "array ::= [[] hws array_16 []] hws \n",
      "string ::= [\"] string_19 [\"] hws \n",
      "number ::= number_20 number_26 number_30 hws \n",
      "value_7 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "hws ::= hws_31 \n",
      "object_9 ::= string [:] hws value object_11 \n",
      "object_10 ::= [,] hws string [:] hws value \n",
      "object_11 ::= object_10 object_11 | \n",
      "object_12 ::= object_9 | \n",
      "array_13 ::= value array_15 \n",
      "array_14 ::= [,] hws value \n",
      "array_15 ::= array_14 array_15 | \n",
      "array_16 ::= array_13 | \n",
      "string_17 ::= [^\"\\] | [\\] string_18 \n",
      "string_18 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_19 ::= string_17 string_19 | \n",
      "number_20 ::= number_21 number_22 \n",
      "number_21 ::= [-] | \n",
      "number_22 ::= [0-9] | [1-9] number_23 \n",
      "number_23 ::= [0-9] number_23 | \n",
      "number_24 ::= [.] number_25 \n",
      "number_25 ::= [0-9] number_25 | [0-9] \n",
      "number_26 ::= number_24 | \n",
      "number_27 ::= [eE] number_28 number_29 \n",
      "number_28 ::= [-+] | \n",
      "number_29 ::= [0-9] number_29 | [0-9] \n",
      "number_30 ::= number_27 | \n",
      "hws_31 ::= [ <U+0009>] hws_31 | \n",
      "{\"things\": [ \"beer\", \"wine\", \"snacks\", \"music\", \"decorations\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   312.98 ms\n",
      "llama_print_timings:      sample time =   172.56 ms /    27 runs   (    6.39 ms per token,   156.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   317.19 ms /    14 tokens (   22.66 ms per token,    44.14 tokens per second)\n",
      "llama_print_timings:        eval time =   709.84 ms /    26 runs   (   27.30 ms per token,    36.63 tokens per second)\n",
      "llama_print_timings:       total time =  1263.05 ms\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Instruction: {question}\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"Create a JSON list of things I need to bring to a party:\"\n",
    "result = llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"things\": [ \"beer\", \"wine\", \"snacks\", \"music\", \"decorations\"]}\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beer', 'wine', 'snacks', 'music', 'decorations']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(result)[\"things\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try `list.gbnf`: \n",
    "\n",
    "* The output will be a list of lines where each line starts with `-`.\n",
    "* Each line contains a sequence of characters that aren't line breaks, followed by a newline character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 6983.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x114550530\n",
      "ggml_metal_init: loaded kernel_add_row                        0x11456c2b0\n",
      "ggml_metal_init: loaded kernel_mul                            0x11455fc40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x114555ee0\n",
      "ggml_metal_init: loaded kernel_scale                          0x114574bf0\n",
      "ggml_metal_init: loaded kernel_silu                           0x11456efd0\n",
      "ggml_metal_init: loaded kernel_relu                           0x114556ec0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x114557120\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x114557380\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x1145575e0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x114557840\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x114570250\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1145704b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x114570710\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x114573f40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1145741a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x114574400\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x114575630\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x114575890\n",
      "ggml_metal_init: loaded kernel_norm                           0x114575af0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x114575d50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x114575fb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x114576210\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x114576470\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1145766d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x114576930\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x114576b90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x114576df0\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x114577050\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x1145772b0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x114577510\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x114577770\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x1145779d0\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x114577c30\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x114577e90\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x1145780f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x114578350\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1145785b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x114578810\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x114578a70\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x114578cd0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =   91.35 MB\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, (14468.72 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.36 MB, (14470.08 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (14872.08 / 21845.34)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    90.02 MB, (14962.09 / 21845.34)\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    "    grammar_path=\"/Users/rlm/Desktop/list.gbnf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n",
      "from_string grammar:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root ::= root_2 \n",
      "item ::= [-] [ ] item_3 [<U+000A>] \n",
      "root_2 ::= item root_2 | item \n",
      "item_3 ::= [^<U+000D><U+000A><U+000B><U+000C><U+0085><U+2028><U+2029>] item_3 | [^<U+000D><U+000A><U+000B><U+000C><U+0085><U+2028><U+2029>] \n",
      "- 1. A bottle of wine. 2. A dessert. 3. A gift for the host. 4.A new outfit.\n",
      "- What you should not bring to a party: - The TV. - Your pet hamster. - Your homework. -Your brother.\n",
      "- Write a paragraph explaining why each of these items should not be brought to a party, using each item on the list.\n",
      "- In your own words, explain why it is inappropriate to bring certain items to a social gathering like a party.\n",
      "- Use specific examples and details to support your explanation.\n",
      "- Include transitions to connect your ideas and make your paragraph flow smoothly.\n",
      "- Read your paragraph aloud to ensure that it sounds natural and makes sense.\n",
      "- Revise your paragraph as needed, using feedback from peers or the teacher to improve clarity and coherence.\n",
      "- Finally, use proper grammar, spelling, and punctuation when writing your paragraph.\n",
      "- Why you should not bring a TV to a party: - If you brought a TV to a party, it would be a distraction and take away from socializing with other guests. - It would also be difficult"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   319.74 ms\n",
      "llama_print_timings:      sample time =  1609.74 ms /   256 runs   (    6.29 ms per token,   159.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =   319.44 ms /    16 tokens (   19.96 ms per token,    50.09 tokens per second)\n",
      "llama_print_timings:        eval time =  7140.70 ms /   255 runs   (   28.00 ms per token,    35.71 tokens per second)\n",
      "llama_print_timings:       total time =  9639.56 ms\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Instruction: {question}\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"Provide a list of things to bring to a party:\"\n",
    "result = llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
