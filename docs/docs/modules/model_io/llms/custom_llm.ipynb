{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9b7651",
   "metadata": {},
   "source": [
    "# Custom LLM\n",
    "\n",
    "This notebook goes over how to create a custom LLM wrapper, in case you want to use your own LLM or a different wrapper than one that is supported in LangChain.\n",
    "\n",
    "Wrapping your LLM with the standard `LLM` interface allow you to use your LLM in existing LangChain programs with minimal code modifications!\n",
    "\n",
    "As an bonus, your LLM will automatically become a LangChain `Runnable` and will benefit from some optimizations out of the box (e.g., batch via a threadpool), async support, the `astream_events` API, etc.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "There are only two required things that a custom LLM needs to implement:\n",
    "\n",
    "- A `_call` method that takes in a string, some optional stop words, and returns a string.\n",
    "- A `_llm_type` property that returns a string. Used for logging purposes only. \n",
    "\n",
    "There is a second optional thing it can implement:\n",
    "\n",
    "- An `_identifying_params` property that is used to help with printing of this class. Should return a dictionary.\n",
    "\n",
    "Let's implement a very simple custom LLM that just returns the first n characters of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a65696a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional, Dict\n",
    "\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5ceff02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
    "\n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = CustomChatModel(n=2)\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "\n",
    "    n: int\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Run the LLM on the given input.\n",
    "\n",
    "        Override this method to implement the LLM logic.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of the stop substrings.\n",
    "                If stop tokens are not supported consider raising NotImplementedError.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            The model output as a string. Actual completions SHOULD NOT include the prompt.\n",
    "        \"\"\"\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return prompt[: self.n]\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "\n",
    "        IMPORTANT:\n",
    "            The LangChain callback system and caching absractions use identifying parameters\n",
    "            to track the configuration with which the LLM call was made.\n",
    "            If identifying parameters are not properly specified, then important tracing\n",
    "            information will be missing in callbacks (e.g., users may not be able to set up\n",
    "            analytics for the model) and caching of model calls will be incorrect.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"n\": self.n,\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": \"CustomChatModel\",\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"custom\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f614fb7b-e476-4d81-821b-57a2ebebe21c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's test it ðŸ§ª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714dede0",
   "metadata": {},
   "source": [
    "This LLM will implement the standard `Runnable` interface of LangChain which many of the LangChain abstractions support!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10e5ece6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = CustomLLM(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36a2da27-03a7-4276-a5d9-59e98ae41eec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCustomLLM\u001b[0m\n",
      "Params: {'n': 3, 'model_name': 'CustomChatModel'}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8cd49199",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thi'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"This is a foobar thing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfebea1",
   "metadata": {},
   "source": [
    "We can also print the LLM and see its custom print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "511b3cb1-9c6f-49b6-9002-a2ec490632b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wor'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await llm.ainvoke('world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9d5bec2-d60a-4ebd-a97d-ac32c98ab02f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woo', 'meo']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.batch(['woof woof woof', 'meow meow meow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe246b29-7a93-4bef-8861-389445598c25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woo', 'meo']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await llm.abatch(['woof woof woof', 'meow meow meow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a67c38f-b83b-4eb9-a231-441c55ee8c82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object BaseLLM.abatch at 0x7805d28a0760>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " llm.abatch(['woof woof woof', 'meow meow meow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950392f-0901-469f-86b0-38b93c820ec9",
   "metadata": {},
   "source": [
    "It'll benfit from other standard runnable methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
