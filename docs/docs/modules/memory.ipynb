{
 "cells": [
  {
   "cell_type": "raw",
   "id": "39402c5b",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 3\n",
    "sidebar_class_name: hidden\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b96a57",
   "metadata": {},
   "source": [
    "\n",
    "# Chat Memory\n",
    "\n",
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation.\n",
    "\n",
    "We call this ability to store information about past interactions \"chat memory\".\n",
    "\n",
    "A chain will interact with its memory system twice in a given run.\n",
    "1. AFTER receiving the initial user inputs but BEFORE executing the core logic, a chain will READ from its memory system and augment the user inputs.\n",
    "2. AFTER executing the core logic but BEFORE returning the answer, a chain will WRITE the inputs and outputs of the current run to memory, so that they can be referred to in future runs.\n",
    "\n",
    "![memory-diagram](/img/memory_diagram.png)\n",
    "\n",
    "\n",
    "## [Chat message storage](docs/integrations/memory/)\n",
    "\n",
    "Underlying any memory is a history of all chat interactions.\n",
    "Even if these are not all used directly, they need to be stored in some form.\n",
    "One of the key parts of the LangChain memory module is a series of integrations for storing these chat messages,\n",
    "from in-memory lists to persistent databases. You can see the full list of these [here](docs/integrations/memory/)\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "We can use the `RunnableWithMessageHistory` class to add chat message memory to any runnable. Let's take a look at a quick example\n",
    "\n",
    "First, let's create a simple chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9035fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b1cbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are helpful chatbot who speaks in pirate speak'),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a55157",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | ChatAnthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6acc9f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Ahoy there matey! Welcome aboard the Jolly Botger! I be Assistant the pirate chatbot at yer service. What adventure can I help ye with on the high seas today?')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hi!\", \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fdfa2e",
   "metadata": {},
   "source": [
    "We now need to choose what we will be using to store our chat history. For this simple example, we will first start with an in-memory store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a395f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36bee9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46996c",
   "metadata": {},
   "source": [
    "We can now wrap this and the chain in `RunnableWithMessageHistory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2e5ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    # This session_id is needed when working with multi-tenant message stores\n",
    "    # where you need to keep track of what session messages belong to.\n",
    "    # Since this is just in memory, it's not needed for this.\n",
    "    lambda session_id: storage,\n",
    "    # This needs to line up with the prompt\n",
    "    input_messages_key=\"input\",\n",
    "    # This needs to line up with the prompt\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8158a501",
   "metadata": {},
   "source": [
    "Whenever we call our chain with message history, we need to include a config that contains the `session_id`\n",
    "\n",
    "Again, this is not needed for our simple in-memory version, but it is needed for multi-tenant applications (which are all production applications) so we make it required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12212bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\": {\"session_id\": \"<SESSION_ID>\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3bdafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" Ahoy Bob! Welcome aboard th' good ship Chatbot! Me name be Captain Chatbot, scourge o' th' seven web seas! What brings ye to me vessel on this fine day? Do ye be needin' help with any scurvy tasks or have any barnacle-covered questions fer ol' Captain Chatbot? Speak up, matey! Ol' Chatbot be ready to assist!\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    {\"input\": \"my name is bob\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a17096b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" Why, yer name be Bob! I'd not be forgettin' the name o' a shipmate so soon. How could I serve ye today, Bob? Need I be steerin' ye toward any particular cove or keepin' a weather eye out for problems that be vexin' ye? Me parrot Squawky and I, we be at yer service! Now, what adventure shall we be undertakin' together on the vast cyber ocean today? Speak up, me hearty!\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    {\"input\": \"what's my name?\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb62a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
